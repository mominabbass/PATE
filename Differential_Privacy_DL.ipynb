{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH9pryo-oHJq"
      },
      "source": [
        "# Deep Learning using Differential Privacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCGCEpAroHJs"
      },
      "source": [
        "## Step 1: Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI75f5-4oHJs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "# Transform the image to a tensor and normalize it\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load the train and test data by using the transform\n",
        "train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crrLASoMoHJt"
      },
      "outputs": [],
      "source": [
        "num_teachers = 100 # Define the num of teachers\n",
        "batch_size = 32 # Teacher batch size\n",
        "\n",
        "def get_data_loaders(train_data, num_teachers):\n",
        "    \"\"\" Function to create data loaders for the Teacher classifier \"\"\"\n",
        "    teacher_loaders = []\n",
        "    data_size = len(train_data) // num_teachers\n",
        "    \n",
        "    for i in range(data_size):\n",
        "        indices = list(range(i*data_size, (i+1)*data_size))\n",
        "        subset_data = Subset(train_data, indices)\n",
        "        loader = torch.utils.data.DataLoader(subset_data, batch_size=batch_size)\n",
        "        teacher_loaders.append(loader)\n",
        "        \n",
        "    return teacher_loaders\n",
        "\n",
        "teacher_loaders = get_data_loaders(train_data, num_teachers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_9-E0AQoHJu"
      },
      "outputs": [],
      "source": [
        "# Create the public dataset by using 90% of the Test data as train data and remaining\n",
        "# 10% as test data.\n",
        "student_train_data = Subset(test_data, list(range(9000)))\n",
        "student_test_data = Subset(test_data, list(range(9000, 10000)))\n",
        "\n",
        "student_train_loader = torch.utils.data.DataLoader(student_train_data, batch_size=batch_size)\n",
        "student_test_loader = torch.utils.data.DataLoader(student_test_data, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gLBmw7boHJu"
      },
      "source": [
        "## Step 2: Define and Train the Teacher models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q3FZF_qoHJu"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    \"\"\" A Simple Feed Forward Neural Network. \n",
        "        A CNN can also be used for this problem \n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16G4RiQDoHJv"
      },
      "outputs": [],
      "source": [
        "def train(model, trainloader, criterion, optimizer, epochs=10):\n",
        "    \"\"\" This function trains a single Classifier model \"\"\"\n",
        "    running_loss = 0\n",
        "    for e in range(epochs):\n",
        "        model.train()\n",
        "        \n",
        "        for images, labels in trainloader:\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            output = model.forward(images)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WzbuAt9oHJy"
      },
      "outputs": [],
      "source": [
        "def predict(model, dataloader):\n",
        "    \"\"\" This function predicts labels for a dataset \n",
        "        given the model and dataloader as inputs. \n",
        "    \"\"\"\n",
        "    outputs = torch.zeros(0, dtype=torch.long)\n",
        "    model.eval()\n",
        "    \n",
        "    for images, labels in dataloader:\n",
        "        output = model.forward(images)\n",
        "        ps = torch.argmax(torch.exp(output), dim=1)\n",
        "        outputs = torch.cat((outputs, ps))\n",
        "        \n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlQNZQ3AoHJz"
      },
      "outputs": [],
      "source": [
        "def train_models(num_teachers):\n",
        "    \"\"\" Trains *num_teacher* models (num_teachers being the number of teacher classifiers) \"\"\"\n",
        "    models = []\n",
        "    for i in range(num_teachers):\n",
        "        model = Classifier()\n",
        "        criterion = nn.NLLLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "        train(model, teacher_loaders[i], criterion, optimizer)\n",
        "        models.append(model)\n",
        "    return models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x717y8-aoHJz",
        "outputId": "dad88242-9a98-45fc-cda4-225b54b0073d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ],
      "source": [
        "models = train_models(num_teachers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSZNBerMoHJ0"
      },
      "source": [
        "## Step 3: Create the Aggregated Teacher and Student Labels by combining the predictions of Teacher models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8h-Pi3WoHJ0"
      },
      "source": [
        "Now, we need to chose the epsilon value for which we first define the formal definition of Differential Privacy\n",
        "\n",
        "![alt text](dp_formula.png \"Title\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBKBepPYoHJ0"
      },
      "source": [
        "This definition does not _create_ differential privacy, instead it is a measure of how much privacy is afforded by a query M. Specifically, it's a comparison between running the query M on a database (x) and a parallel database (y). As you remember, parallel databases are defined to be the same as a full database (x) with one entry/person removed.\n",
        "\n",
        "Thus, this definition says that FOR ALL parallel databases, the maximum distance between a query on database (x) and the same query on database (y) will be e^epsilon, but that occasionally this constraint won't hold with probability delta. Thus, this theorem is called \"epsilon delta\" differential privacy.\n",
        "\n",
        "\n",
        "\n",
        "### How much noise should we add?\n",
        "\n",
        "The amount of noise necessary to add to the output of a query is a function of four things:\n",
        "\n",
        "- the type of noise (Gaussian/Laplacian)\n",
        "- the sensitivity of the query/function\n",
        "- the desired epsilon (ε)\n",
        "- the desired delta (δ)\n",
        "\n",
        "Thus, for each type of noise we're adding, we have different way of calculating how much to add as a function of sensitivity, epsilon, and delta. We're going to focus on Laplacian noise. Laplacian noise is increased/decreased according to a \"scale\" parameter b. We choose \"b\" based on the following formula.\n",
        "\n",
        "b = sensitivity(query) / epsilon\n",
        "\n",
        "In other words, if we set b to be this value, then we know that we will have a privacy leakage of <= epsilon. Furthermore, the nice thing about Laplace is that it guarantees this with delta == 0. There are some tunings where we can have very low epsilon where delta is non-zero, but we'll ignore them for now.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7wEC37hoHJ1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "epsilon = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FU5w3t0oHJ1"
      },
      "outputs": [],
      "source": [
        "def aggregated_teacher(models, dataloader, epsilon):\n",
        "    \"\"\" Take predictions from individual teacher model and \n",
        "        creates the true labels for the student after adding \n",
        "        laplacian noise to them \n",
        "    \"\"\"\n",
        "    preds = torch.torch.zeros((len(models), 9000), dtype=torch.long)\n",
        "    for i, model in enumerate(models):\n",
        "        results = predict(model, dataloader)\n",
        "        preds[i] = results\n",
        "    \n",
        "    labels = np.array([]).astype(int)\n",
        "    for image_preds in np.transpose(preds):\n",
        "        label_counts = np.bincount(image_preds, minlength=10)\n",
        "        beta = 1 / epsilon\n",
        "\n",
        "        for i in range(len(label_counts)):\n",
        "            label_counts[i] += np.random.laplace(0, beta, 1)\n",
        "\n",
        "        new_label = np.argmax(label_counts)\n",
        "        labels = np.append(labels, new_label)\n",
        "    \n",
        "    return preds.numpy(), labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJtKGjmWoHJ1",
        "outputId": "8ae3cf8f-2d85-42f5-e475-d923090b2902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ],
      "source": [
        "teacher_models = models\n",
        "preds, student_labels = aggregated_teacher(teacher_models, student_train_loader, epsilon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSSvLJsSoHJ1"
      },
      "source": [
        "## Step 4: Create the Student model and train it using the labels generated in step 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AZ7mPHeoHJ1"
      },
      "outputs": [],
      "source": [
        "def student_loader(student_train_loader, labels):\n",
        "    for i, (data, _) in enumerate(iter(student_train_loader)):\n",
        "        yield data, torch.from_numpy(labels[i*len(data): (i+1)*len(data)])\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpU56JNcoHJ2",
        "outputId": "3b70f981-6f66-4f02-8f98-b08c1c3e4d81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10..  Training Loss: 0.298..  Test Loss: 0.850..  Test Accuracy: 0.727\n",
            "Epoch: 1/10..  Training Loss: 0.135..  Test Loss: 0.401..  Test Accuracy: 0.898\n",
            "Epoch: 1/10..  Training Loss: 0.100..  Test Loss: 0.306..  Test Accuracy: 0.908\n",
            "Epoch: 1/10..  Training Loss: 0.074..  Test Loss: 0.300..  Test Accuracy: 0.919\n",
            "Epoch: 1/10..  Training Loss: 0.067..  Test Loss: 0.252..  Test Accuracy: 0.933\n",
            "Epoch: 2/10..  Training Loss: 0.116..  Test Loss: 0.374..  Test Accuracy: 0.914\n",
            "Epoch: 2/10..  Training Loss: 0.088..  Test Loss: 0.234..  Test Accuracy: 0.931\n",
            "Epoch: 2/10..  Training Loss: 0.070..  Test Loss: 0.234..  Test Accuracy: 0.929\n",
            "Epoch: 2/10..  Training Loss: 0.065..  Test Loss: 0.200..  Test Accuracy: 0.938\n",
            "Epoch: 2/10..  Training Loss: 0.048..  Test Loss: 0.192..  Test Accuracy: 0.941\n",
            "Epoch: 2/10..  Training Loss: 0.039..  Test Loss: 0.188..  Test Accuracy: 0.955\n",
            "Epoch: 3/10..  Training Loss: 0.131..  Test Loss: 0.221..  Test Accuracy: 0.938\n",
            "Epoch: 3/10..  Training Loss: 0.064..  Test Loss: 0.177..  Test Accuracy: 0.952\n",
            "Epoch: 3/10..  Training Loss: 0.066..  Test Loss: 0.181..  Test Accuracy: 0.947\n",
            "Epoch: 3/10..  Training Loss: 0.049..  Test Loss: 0.163..  Test Accuracy: 0.955\n",
            "Epoch: 3/10..  Training Loss: 0.045..  Test Loss: 0.160..  Test Accuracy: 0.960\n",
            "Epoch: 4/10..  Training Loss: 0.067..  Test Loss: 0.211..  Test Accuracy: 0.948\n",
            "Epoch: 4/10..  Training Loss: 0.072..  Test Loss: 0.169..  Test Accuracy: 0.955\n",
            "Epoch: 4/10..  Training Loss: 0.054..  Test Loss: 0.165..  Test Accuracy: 0.949\n",
            "Epoch: 4/10..  Training Loss: 0.058..  Test Loss: 0.162..  Test Accuracy: 0.960\n",
            "Epoch: 4/10..  Training Loss: 0.038..  Test Loss: 0.168..  Test Accuracy: 0.961\n",
            "Epoch: 4/10..  Training Loss: 0.046..  Test Loss: 0.163..  Test Accuracy: 0.958\n",
            "Epoch: 5/10..  Training Loss: 0.081..  Test Loss: 0.230..  Test Accuracy: 0.948\n",
            "Epoch: 5/10..  Training Loss: 0.061..  Test Loss: 0.158..  Test Accuracy: 0.957\n",
            "Epoch: 5/10..  Training Loss: 0.051..  Test Loss: 0.166..  Test Accuracy: 0.955\n",
            "Epoch: 5/10..  Training Loss: 0.046..  Test Loss: 0.193..  Test Accuracy: 0.948\n",
            "Epoch: 5/10..  Training Loss: 0.042..  Test Loss: 0.182..  Test Accuracy: 0.953\n",
            "Epoch: 5/10..  Training Loss: 0.032..  Test Loss: 0.212..  Test Accuracy: 0.945\n",
            "Epoch: 6/10..  Training Loss: 0.079..  Test Loss: 0.186..  Test Accuracy: 0.951\n",
            "Epoch: 6/10..  Training Loss: 0.052..  Test Loss: 0.182..  Test Accuracy: 0.953\n",
            "Epoch: 6/10..  Training Loss: 0.048..  Test Loss: 0.186..  Test Accuracy: 0.953\n",
            "Epoch: 6/10..  Training Loss: 0.040..  Test Loss: 0.157..  Test Accuracy: 0.958\n",
            "Epoch: 6/10..  Training Loss: 0.033..  Test Loss: 0.167..  Test Accuracy: 0.959\n",
            "Epoch: 7/10..  Training Loss: 0.061..  Test Loss: 0.197..  Test Accuracy: 0.951\n",
            "Epoch: 7/10..  Training Loss: 0.056..  Test Loss: 0.174..  Test Accuracy: 0.956\n",
            "Epoch: 7/10..  Training Loss: 0.050..  Test Loss: 0.169..  Test Accuracy: 0.957\n",
            "Epoch: 7/10..  Training Loss: 0.052..  Test Loss: 0.155..  Test Accuracy: 0.958\n",
            "Epoch: 7/10..  Training Loss: 0.030..  Test Loss: 0.163..  Test Accuracy: 0.954\n",
            "Epoch: 7/10..  Training Loss: 0.033..  Test Loss: 0.157..  Test Accuracy: 0.961\n",
            "Epoch: 8/10..  Training Loss: 0.067..  Test Loss: 0.193..  Test Accuracy: 0.958\n",
            "Epoch: 8/10..  Training Loss: 0.053..  Test Loss: 0.161..  Test Accuracy: 0.958\n",
            "Epoch: 8/10..  Training Loss: 0.043..  Test Loss: 0.172..  Test Accuracy: 0.956\n",
            "Epoch: 8/10..  Training Loss: 0.041..  Test Loss: 0.169..  Test Accuracy: 0.958\n",
            "Epoch: 8/10..  Training Loss: 0.031..  Test Loss: 0.173..  Test Accuracy: 0.959\n",
            "Epoch: 8/10..  Training Loss: 0.027..  Test Loss: 0.196..  Test Accuracy: 0.955\n",
            "Epoch: 9/10..  Training Loss: 0.100..  Test Loss: 0.196..  Test Accuracy: 0.955\n",
            "Epoch: 9/10..  Training Loss: 0.045..  Test Loss: 0.172..  Test Accuracy: 0.956\n",
            "Epoch: 9/10..  Training Loss: 0.046..  Test Loss: 0.169..  Test Accuracy: 0.955\n",
            "Epoch: 9/10..  Training Loss: 0.034..  Test Loss: 0.194..  Test Accuracy: 0.949\n",
            "Epoch: 9/10..  Training Loss: 0.025..  Test Loss: 0.194..  Test Accuracy: 0.957\n",
            "Epoch: 10/10..  Training Loss: 0.062..  Test Loss: 0.217..  Test Accuracy: 0.952\n",
            "Epoch: 10/10..  Training Loss: 0.054..  Test Loss: 0.179..  Test Accuracy: 0.954\n",
            "Epoch: 10/10..  Training Loss: 0.042..  Test Loss: 0.169..  Test Accuracy: 0.959\n",
            "Epoch: 10/10..  Training Loss: 0.049..  Test Loss: 0.170..  Test Accuracy: 0.956\n",
            "Epoch: 10/10..  Training Loss: 0.028..  Test Loss: 0.194..  Test Accuracy: 0.960\n",
            "Epoch: 10/10..  Training Loss: 0.032..  Test Loss: 0.158..  Test Accuracy: 0.968\n"
          ]
        }
      ],
      "source": [
        "student_model = Classifier()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(student_model.parameters(), lr=0.003)\n",
        "epochs = 10\n",
        "steps = 0\n",
        "running_loss = 0\n",
        "for e in range(epochs):\n",
        "    student_model.train()\n",
        "    train_loader = student_loader(student_train_loader, student_labels)\n",
        "    for images, labels in train_loader:\n",
        "        steps += 1\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = student_model.forward(images)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if steps % 50 == 0:\n",
        "            test_loss = 0\n",
        "            accuracy = 0\n",
        "            student_model.eval()\n",
        "            with torch.no_grad():\n",
        "                for images, labels in student_test_loader:\n",
        "                    log_ps = student_model(images)\n",
        "                    test_loss += criterion(log_ps, labels).item()\n",
        "                    \n",
        "                    # Accuracy\n",
        "                    ps = torch.exp(log_ps)\n",
        "                    top_p, top_class = ps.topk(1, dim=1)\n",
        "                    equals = top_class == labels.view(*top_class.shape)\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "            student_model.train()\n",
        "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "                  \"Training Loss: {:.3f}.. \".format(running_loss/len(student_train_loader)),\n",
        "                  \"Test Loss: {:.3f}.. \".format(test_loss/len(student_test_loader)),\n",
        "                  \"Test Accuracy: {:.3f}\".format(accuracy/len(student_test_loader)))\n",
        "            running_loss = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfMONCzJoHJ2"
      },
      "source": [
        "## Step 5: Let's Perform PATE Analysis on the student labels generated by the Aggregated Teacher"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install syft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YeeWYnYgNEF4",
        "outputId": "97c72d21-7c22-4ed7-94e5-885c7f9fa7ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting syft\n",
            "  Downloading syft-0.6.0-py2.py3-none-any.whl (606 kB)\n",
            "\u001b[K     |████████████████████████████████| 606 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting SQLAlchemy==1.4.27\n",
            "  Downloading SQLAlchemy-1.4.27-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 21.9 MB/s \n",
            "\u001b[?25hCollecting sympy==1.9\n",
            "  Downloading sympy-1.9-py3-none-any.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 45.3 MB/s \n",
            "\u001b[?25hCollecting requests-toolbelt==0.9.1\n",
            "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.8 MB/s \n",
            "\u001b[?25hCollecting typing-extensions==4.0.0\n",
            "  Downloading typing_extensions-4.0.0-py3-none-any.whl (22 kB)\n",
            "Collecting loguru==0.5.3\n",
            "  Downloading loguru-0.5.3-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting numpy==1.21.4\n",
            "  Downloading numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 36.8 MB/s \n",
            "\u001b[?25hCollecting PyJWT==2.3.0\n",
            "  Downloading PyJWT-2.3.0-py3-none-any.whl (16 kB)\n",
            "Collecting requests==2.26.0\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 886 kB/s \n",
            "\u001b[?25hCollecting bcrypt==3.2.0\n",
            "  Downloading bcrypt-3.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 445 kB/s \n",
            "\u001b[?25hCollecting ascii-magic==1.6\n",
            "  Downloading ascii_magic-1.6-py3-none-any.whl (7.6 kB)\n",
            "Collecting protobuf==3.19.1\n",
            "  Downloading protobuf-3.19.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 51.1 MB/s \n",
            "\u001b[?25hCollecting forbiddenfruit==0.1.4\n",
            "  Downloading forbiddenfruit-0.1.4.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting pandas==1.3.4\n",
            "  Downloading pandas-1.3.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 39.2 MB/s \n",
            "\u001b[?25hCollecting pydantic[email]==1.8.2\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 47.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<=1.10.0,>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from syft) (1.10.0+cu111)\n",
            "Collecting packaging==21.2\n",
            "  Downloading packaging-21.2-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting gevent==21.8.0\n",
            "  Downloading gevent-21.8.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 23.2 MB/s \n",
            "\u001b[?25hCollecting PyNaCl==1.4.0\n",
            "  Downloading PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961 kB)\n",
            "\u001b[K     |████████████████████████████████| 961 kB 35.4 MB/s \n",
            "\u001b[?25hCollecting names==0.3.0\n",
            "  Downloading names-0.3.0.tar.gz (789 kB)\n",
            "\u001b[K     |████████████████████████████████| 789 kB 60.3 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata==4.8.2\n",
            "  Downloading importlib_metadata-4.8.2-py3-none-any.whl (17 kB)\n",
            "Collecting pymbolic==2021.1\n",
            "  Downloading pymbolic-2021.1.tar.gz (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting werkzeug==2.0.2\n",
            "  Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
            "\u001b[K     |████████████████████████████████| 288 kB 48.4 MB/s \n",
            "\u001b[?25hCollecting autodp==0.2\n",
            "  Downloading autodp-0.2.tar.gz (39 kB)\n",
            "Collecting tqdm==4.62.3\n",
            "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools==4.2.4 in /usr/local/lib/python3.7/dist-packages (from syft) (4.2.4)\n",
            "Collecting pyarrow==6.0.0\n",
            "  Downloading pyarrow-6.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.5 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from ascii-magic==1.6->syft) (7.1.2)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from autodp==0.2->syft) (1.4.1)\n",
            "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from bcrypt==3.2.0->syft) (1.15.0)\n",
            "Requirement already satisfied: cffi>=1.1 in /usr/local/lib/python3.7/dist-packages (from bcrypt==3.2.0->syft) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from gevent==21.8.0->syft) (57.4.0)\n",
            "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gevent==21.8.0->syft) (1.1.2)\n",
            "Collecting zope.interface\n",
            "  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 67.1 MB/s \n",
            "\u001b[?25hCollecting zope.event\n",
            "  Downloading zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata==4.8.2->syft) (3.7.0)\n",
            "Collecting pyparsing<3,>=2.0.2\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.4->syft) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.4->syft) (2.8.2)\n",
            "Collecting email-validator>=1.0.3\n",
            "  Downloading email_validator-1.1.3-py2.py3-none-any.whl (18 kB)\n",
            "Collecting pytools>=2\n",
            "  Downloading pytools-2022.1.2.tar.gz (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest>=2.3 in /usr/local/lib/python3.7/dist-packages (from pymbolic==2021.1->syft) (3.6.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.26.0->syft) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.26.0->syft) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.26.0->syft) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests==2.26.0->syft) (2.0.12)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy==1.9->syft) (1.2.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1->bcrypt==3.2.0->syft) (2.21)\n",
            "Collecting dnspython>=1.15.0\n",
            "  Downloading dnspython-2.2.1-py3-none-any.whl (269 kB)\n",
            "\u001b[K     |████████████████████████████████| 269 kB 56.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=2.3->pymbolic==2021.1->syft) (8.12.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=2.3->pymbolic==2021.1->syft) (21.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest>=2.3->pymbolic==2021.1->syft) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=2.3->pymbolic==2021.1->syft) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=2.3->pymbolic==2021.1->syft) (1.11.0)\n",
            "Collecting platformdirs>=2.2.0\n",
            "  Downloading platformdirs-2.5.1-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: autodp, forbiddenfruit, names, pymbolic, pytools\n",
            "  Building wheel for autodp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autodp: filename=autodp-0.2-py3-none-any.whl size=42458 sha256=dfe8cb85af9e0f0c12b08aecda9916bbcc022cafe516feb2b27ba388bf06c11f\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/95/85/6a1fc999268b5ed5aaeb387ff91f0063698b8a4db55e5c38c5\n",
            "  Building wheel for forbiddenfruit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for forbiddenfruit: filename=forbiddenfruit-0.1.4-py3-none-any.whl size=21810 sha256=6c5e61a3c1ee74873dbcbf31f294abd8268b6ae4c147b7eeae93d0c13e68a1f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b1/d4/d715c0964da2cbf9f773760b9cf9352405eb4e6a988ffd2606\n",
            "  Building wheel for names (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for names: filename=names-0.3.0-py3-none-any.whl size=803699 sha256=c797a8877ca6d002dacd3750368d22fd964da1678f681832fcbc7d4103cf97f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/ea/68/92f6b0669e478af9b7c3c524520d03050089e034edcc775c2b\n",
            "  Building wheel for pymbolic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymbolic: filename=pymbolic-2021.1-py3-none-any.whl size=110410 sha256=a5090c30032bcac7004d871043bd5f00662166c5d068bd210502e9b6082e5827\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/f9/34/d75a7263fdb5fd102d58693151064e229acdf1a8640f91e5be\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2022.1.2-py2.py3-none-any.whl size=63903 sha256=557e7052ee25f0935e3c4698a07ece694eabd2fb7f2523a435434a80efdae779\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/24/3d/aa7df2a581a0cc0c86ca368104c8e3613848c78df40a88e9f9\n",
            "Successfully built autodp forbiddenfruit names pymbolic pytools\n",
            "Installing collected packages: typing-extensions, platformdirs, numpy, dnspython, zope.interface, zope.event, requests, pytools, pyparsing, pydantic, importlib-metadata, email-validator, colorama, werkzeug, tqdm, sympy, SQLAlchemy, requests-toolbelt, PyNaCl, pymbolic, PyJWT, pyarrow, protobuf, pandas, packaging, names, loguru, gevent, forbiddenfruit, bcrypt, autodp, ascii-magic, syft\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.7\n",
            "    Uninstalling pyparsing-3.0.7:\n",
            "      Successfully uninstalled pyparsing-3.0.7\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.11.3\n",
            "    Uninstalling importlib-metadata-4.11.3:\n",
            "      Successfully uninstalled importlib-metadata-4.11.3\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.63.0\n",
            "    Uninstalling tqdm-4.63.0:\n",
            "      Successfully uninstalled tqdm-4.63.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.7.1\n",
            "    Uninstalling sympy-1.7.1:\n",
            "      Successfully uninstalled sympy-1.7.1\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 1.4.32\n",
            "    Uninstalling SQLAlchemy-1.4.32:\n",
            "      Successfully uninstalled SQLAlchemy-1.4.32\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 6.0.1\n",
            "    Uninstalling pyarrow-6.0.1:\n",
            "      Successfully uninstalled pyarrow-6.0.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 21.3\n",
            "    Uninstalling packaging-21.3:\n",
            "      Successfully uninstalled packaging-21.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "flask 1.1.4 requires Werkzeug<2.0,>=0.15, but you have werkzeug 2.0.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "arviz 0.11.4 requires typing-extensions<4,>=3.7.4.3, but you have typing-extensions 4.0.0 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed PyJWT-2.3.0 PyNaCl-1.4.0 SQLAlchemy-1.4.27 ascii-magic-1.6 autodp-0.2 bcrypt-3.2.0 colorama-0.4.4 dnspython-2.2.1 email-validator-1.1.3 forbiddenfruit-0.1.4 gevent-21.8.0 importlib-metadata-4.8.2 loguru-0.5.3 names-0.3.0 numpy-1.21.4 packaging-21.2 pandas-1.3.4 platformdirs-2.5.1 protobuf-3.19.1 pyarrow-6.0.0 pydantic-1.8.2 pymbolic-2021.1 pyparsing-2.4.7 pytools-2022.1.2 requests-2.26.0 requests-toolbelt-0.9.1 syft-0.6.0 sympy-1.9 tqdm-4.62.3 typing-extensions-4.0.0 werkzeug-2.0.2 zope.event-4.5.0 zope.interface-5.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy",
                  "packaging",
                  "pandas",
                  "pyarrow",
                  "pyparsing",
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "c2VQkhCgoHJ2",
        "outputId": "d12e80a4-3678-4750-a83e-0ac0a4ca7d61"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-11897be79f76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msyft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeworks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifferential_privacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_dep_eps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_ind_eps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher_preds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstudent_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_eps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data Independent Epsilon:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_ind_eps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data Dependent Epsilon:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dep_eps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'syft.frameworks'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from syft.frameworks.torch.differential_privacy import pate\n",
        "\n",
        "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=preds, indices=student_labels, noise_eps=epsilon, delta=1e-5)\n",
        "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
        "print(\"Data Dependent Epsilon:\", data_dep_eps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3bHlQd5oHJ2"
      },
      "source": [
        "The pate.perform_analysis method returns two values - a data independent epsilon and a data dependent epsilon. The data dependent epsilon is the epsilon value obtained by looking at how much the teachers agree with each other. In a way, the PATE analysis rewards the user for building teacher models which agree with each other because it becomes harder to leak information and track individual information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVFfjTPYoHJ3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSBSFxOYoHJ3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Differential Privacy DL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}